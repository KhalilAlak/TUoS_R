hist2<-ggplot(mpg, aes(cty))+
geom_histogram(binwidth=5, fill="lightblue")+ labs(x="Fuel economy in the city",
y="Frequency", caption="mpg dataset", title='Binwidth')
hist3<-ggplot(mpg, aes(cty))+
geom_histogram(breaks=c(5,10,15,20,25,30,35,40), fill="lightcoral")+
labs(x="Fuel economy in the city", y="Frequency",
caption="mpg dataset", title='Breaks')
grid.arrange(hist1, hist2, hist3, ncol=1)
#Execise 7
ggplot(mpg, aes(x = displ)) +
geom_histogram(breaks = c(1, 3, 5, 7), fill = "skyblue", color = "black") +
labs(
title = "Histogram with Custom Bin Boundaries",
x = "Engine Displacement",
y = "Frequency",
caption = "mpg dataset"
)
ggplot(mpg, aes(x = displ)) +
geom_histogram(bins = 15, fill = "lightgreen", color = "black") +
labs(
title = "Histogram with 15 Bins",
x = "Engine Displacement",
y = "Frequency",
caption = "mpg dataset"
)
ggplot(mpg, aes(x = displ)) +
geom_histogram(binwidth = 1, fill = "salmon", color = "black") +
labs(
title = "Histogram with Bin Width = 1",
x = "Engine Displacement",
y = "Frequency",
caption = "mpg dataset"
)
ggplot(mpg, aes(x = cty, y = hwy)) +
geom_bin2d(binwidth = c(2, 2)) +
scale_fill_gradient(low = "lightblue", high = "darkblue") +
labs(
title = "2D Histogram of City vs Highway MPG",
x = "City MPG",
y = "Highway MPG",
fill = "Count",
caption = "mpg dataset"
)
ggplot(mpg, aes(class, cty)) +
geom_boxplot(varwidth=TRUE, fill="plum") +
labs(title="Fuel economy in city grouped by Class of vehicle",
caption="mpg dataset",
x="Class of Vehicle",
y="City Mileage")
#Execise 8
ggplot(mpg, aes(x = factor(cyl), y = displ)) +
geom_boxplot(varwidth = TRUE, fill = "lightblue") +
labs(
title = "Engine Displacement by Cylinder Count",
x = "Number of Cylinders",
y = "Engine Displacement",
caption = "mpg dataset"
)
ggplot(mpg, aes(x = factor(cyl), y = displ)) +
geom_violin(fill = "orchid", trim = FALSE) +
labs(
title = "Engine Displacement by Cylinder Count (Violin Plot)",
x = "Number of Cylinders",
y = "Engine Displacement",
caption = "mpg dataset"
)
ggplot(mpg, aes(x = factor(cyl), y = displ)) +
geom_violin(fill = "lightgray", trim = FALSE) +
geom_boxplot(width = 0.1, fill = "blue", outlier.shape = NA) +
labs(
title = "Combined Violin and Boxplot",
x = "Number of Cylinders",
y = "Engine Displacement",
caption = "mpg dataset"
)
#Execise 9
ggplot(diamonds, aes(x = carat, y = price, color = cut, size = clarity)) +
geom_point(alpha = 0.6) +
labs(
title = "Diamond Price vs Carat",
x = "Carat",
y = "Price (USD)",
color = "Cut",
size = "Clarity"
) +
theme_minimal()
ggplot(diamonds, aes(x = cut, y = price, fill = cut)) +
geom_boxplot() +
labs(
title = "Price Distribution by Cut",
x = "Cut Quality",
y = "Price (USD)",
fill = "Cut"
) +
theme_minimal()
install.packages('tm')
install.packages('snowballC')
install.packages('glmnet')
library(tm)
library(glmnet)
library(SnowballC)
install.packages('SnowballC')
library(SnowballC)
topic_tocs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
setwd("~/TUoS_R/Week 7")
topic_tocs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
setwd("~/TUoS_R/Week 7")
topic_tocs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
View(topic_tocs)
summary(topic_docs[1:5])
summary(topic_docs[1:5])
summary(topic_docs[1:5])
library(tm)
library(glmnet)
library(SnowballC)
topic_tocs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
summary(topic_docs[1:5])
topic_tocs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
summary(topic_docs[1:5])
setwd("~/TUoS_R/Week 7")
topic_tocs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
summary(topic_docs[1:5])
topic_docs[1:5]
library(tm)
library(glmnet)
library(SnowballC)
library(tidyverse)
topic_tocs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
summary(topic_docs[1:5])
topic_docs <- Corpus(
DirSource(
"20news-train/comp.graphics",
encoding='UTF-8'
)
)
summary(topic_docs[1:5])
inspect(topics_docs[[1]])
inspect(topic_docs[[1]])
topic_docs[[1]]$meta
topic_docs[[1]]$content
topic_2_docs <- Corpus(
DirSource(
"20news-train/rec.motorcycles",
encoding='UTF-8'
)
)
binomial_docs <- c(
as.list(topic_docs), # we need to convert the Corpus to a list to combine t
hem properly
binomial_docs <- c(
as.list(topic_docs),
hem properly
topic_2_docs <- Corpus(
DirSource(
"20news-train/rec.motorcycles",
encoding='UTF-8'
)
)
binomial_docs <- c(
as.list(topic_docs),
hem properly
binomial_docs <- c(
as.list(topic_doc),
hem properly
binomial_docs <- c(
as.list(topic_docs),
hem properly
binomial_docs <- c(
as.list(topic_docs),
as.list(topic_2_docs)
)
labels_1 <- replicate(length(topic_docs), 'comp.graphics')
labels_2 <- replicate(length(topic_2_docs), 'rec.motorcycles')
binomial_labels <- c(labels_1, labels_2)
length(binomial_docs) == length(binomial_labels)
example_doc <- binomial_docs[["38487"]]
print(example_doc)
tokens <- Boost_tokenizer(example_doc)
summary(tokens)
tokens <- MC_tokenizer(example_doc)
summary(tokens)
view("MC_tokenizer(example_doc)")
view("tokens")
view(tokens)
summary(tokens)
view(tokens)
tokens <- Boost_tokenizer(example_doc)
summary(tokens)
view(tokens)
tokens <- MC_tokenizer(example_doc)
summary(tokens)
view(tokens)
removePunctuation(example_doc)
removePunctuation(tokens)
stops <- stopwords('en')
print(stops)
removeWords(example_doc, stops)
tolower(example_doc)
removeWords(
tolower(example_doc),
stops
)
#Execise 2
example_doc <- "Subject: WANTED: Multi-page GIF!! Hi!... I am searching for packages that could handle Multi-page GIF files... Are there any on some ftp servers? Iâ€™ll appreciate one which works on PC (either on DOS or Windows 3.0/3.1). But any package works on Unix will be OK.. Thanks in advance..."
tokens <- strsplit(example_doc, "\\s+")[[1]]
print(tokens[1:20])  # view first 20 tokens
tokens_lower <- tolower(tokens)
library(tm)
stops <- stopwords("en")
tokens_clean <- removeWords(tokens_lower, stops)
print(tokens_clean[1:20])
removeWords(
tolower(tokens),
stops
)
stemDocument(
example_doc
)
#Execise 2
tokens <- strsplit(example_doc, "\\s+")[[1]]
print(tokens[1:20])
tokens_lower <- tolower(tokens)
library(tm)
stops <- stopwords("en")
tokens_clean <- removeWords(tokens_lower, stops)
print(tokens_clean[1:20])
removeWords(
tolower(tokens),
stops
)
stemDocument(
example_doc
)
text_lower <- tolower(example_doc)
text_nopunct <- removePunctuation(text_lower)
stops <- stopwords("en")
text_nostop <- removeWords(text_nopunct, stops)
print(text_stemmed)
text_stemmed <- stemDocument(text_nostop)
text_stemmed <- stemDocument(text_nostop)
print(text_stemmed)
binomial_docs <- Corpus(
VectorSource(binomial_docs)
)
binomial_docs
#Execise 1
tokens <- MC_tokenizer(example_doc)
summary(tokens)
view(tokens)
tokens <- Boost_tokenizer(example_doc)
summary(tokens)
view(tokens)
#Execise 1
tokens <- MC_tokenizer(example_doc)
summary(tokens)
view(tokens)
cleaned_binomial_docs <- tm_map(
binomial_docs,
tolower
)
cleaned_binomial_docs$content[1]
binomial_docs <- Corpus(
VectorSource(binomial_docs)
)
binomial_docs
# Start with lowercasing
cleaned_binomial_docs <- tm_map(
binomial_docs, # the collection of documents to process
tolower # the function to apply to each document
)
cleaned_binomial_docs$content[1] # view the document - are they in lowercase now?
cleaned_binomial_docs$content[1]
# Then remove punctuation
cleaned_binomial_docs <- tm_map(
cleaned_binomial_docs, # we want to stack on top of the previous preprocessing!
removePunctuation # the function to apply to each document
)
cleaned_binomial_docs$content[1] # have all punctuations been removed?
# Then remove stopwords
cleaned_binomial_docs <- tm_map(
cleaned_binomial_docs,
removeWords, # the function to apply to each document
stopwords('en') # additional argument to specify the stopwords to be removed
)
cleaned_binomial_docs$content[1] # have the stopwords been removed?
# And finally stem
binomial_docs <- Corpus(
VectorSource(binomial_docs)
)
binomial_docs
# Start with lowercasing
cleaned_binomial_docs <- tm_map(
binomial_docs, # the collection of documents to process
tolower # the function to apply to each document
)
cleaned_binomial_docs$content[1] # view the document - are they in lowercase now?
cleaned_binomial_docs$content[1]
# Then remove punctuation
cleaned_binomial_docs <- tm_map(
cleaned_binomial_docs, # we want to stack on top of the previous preprocessing!
removePunctuation # the function to apply to each document
)
cleaned_binomial_docs$content[1] # have all punctuations been removed?
# Then remove stopwords
cleaned_binomial_docs <- tm_map(
cleaned_binomial_docs,
removeWords, # the function to apply to each document
stopwords('en') # additional argument to specify the stopwords to be removed
)
cleaned_binomial_docs$content[1] # have the stopwords been removed?
# And finally stem
cleaned_binomial_docs <- tm_map(
cleaned_binomial_docs,
stemDocument
)
cleaned_binomial_docs$content[1] # have the words been stemmed?
binomial_dtm <- DocumentTermMatrix(
cleaned_binomial_docs
)
binomial_dtm
binomial_dtm <- DocumentTermMatrix(cleaned_binomial_docs)
original_dtm <- DocumentTermMatrix(binomial_docs)
original_dtm
inspect(binomial_dtm[1:3,])
doc_lengths <- lapply( # applies a function across each element of a list
as.list(cleaned_binomial_docs),
nchar # count the length of a string
)
doc_lengths <- unlist(doc_lengths) # get rid of the structure that lapply()
creates
doc_lengths <- unlist(doc_lengths) # get rid of the structure that lapply() creates
quantile(doc_lengths)
omial_dtm_binary <- DocumentTermMatrix(
cleaned_binomial_docs,
control=list(
weighting=weightBin
)
)
binomial_dtm_binary <- DocumentTermMatrix(
cleaned_binomial_docs,
control=list(
weighting=weightBin
)
)
inspect(binomial_dtm_binary[1:3,])
binomial_dtm_tfidf <- DocumentTermMatrix(
cleaned_binomial_docs,
control=list(
weighting=weightTfIdf
)
)
inspect(binomial_dtm_tfidf[1:3,])
removeSparseTerms(binomial_dtm, 0.98)
#Execise 5
removeSparseTerms(dtm, sparse = x)
#Execise 5
removeSparseTerms(dtm, sparse = x)
removeSparseTerms(dtm, sparse = x)
removeSparseTerms(dtm, 0.98)
#Execise 5
library(tm)
# Step 1: Create a text corpus (example)
# Replace 'cleaned_binomial_docs' with your actual corpus variable
corpus <- cleaned_binomial_docs
# Step 2: Create the Document-Term Matrix
dtm <- DocumentTermMatrix(corpus)
# Step 3: Check the DTM
dtm
# Step 4: Now you can remove sparse terms safely
dtm_sparse <- removeSparseTerms(dtm, 0.98)
dtm_sparse
removeSparseTerms(dtm, sparse = x)
removeSparseTerms(dtm, sparse = 0.90)
removeSparseTerms(dtm, sparse = 0.95)
binomial_dtm
ncol(removeSparseTerms(binomial_dtm, 0.99))   # threshold = 0.99
ncol(removeSparseTerms(binomial_dtm, 0.98))   # threshold = 0.98
ncol(removeSparseTerms(binomial_dtm, 0.95))   # threshold = 0.95
ncol(removeSparseTerms(binomial_dtm, 0.90))   # threshold = 0.90
ncol(removeSparseTerms(binomial_dtm, 0.80))   # threshold = 0.80
observed_vocabulary <- unlist(binomial_dtm$dimnames['Terms'])
ial_train_dtm <- DocumentTermMatrix(
cleaned_binomial_docs,
control=list(
dictionary=observed_vocabulary
)
)
binomial_train_labels <- (
binomial_labels == 'comp.graphics'
) * 1
binomial_model <- glmnet(
binomial_train_dtm,
binomial_train_labels,
family='binomial'
)
binomial_train_labels <- (
binomial_labels == 'comp.graphics'
) * 1
binomial_model <- glmnet(
binomial_train_dtm,
binomial_train_labels,
family='binomial'
)
binomial_train_dtm <- DocumentTermMatrix(
cleaned_binomial_docs,
control=list(
dictionary=observed_vocabulary
)
)
binomial_train_labels <- (
binomial_labels == 'comp.graphics'
) * 1
binomial_model <- glmnet(
binomial_train_dtm,
binomial_train_labels,
family='binomial'
)
observed_vocabulary <- unlist(binomial_dtm$dimnames['Terms'])
binomial_train_dtm <- DocumentTermMatrix(
cleaned_binomial_docs,
control=list(
dictionary=observed_vocabulary
)
)
binomial_train_labels <- (
binomial_labels == 'comp.graphics'
) * 1
binomial_model <- glmnet(
binomial_train_dtm,
binomial_train_labels,
family='binomial'
)
observed_vocabulary <- unlist(binomial_dtm$dimnames['Terms'])
observed_vocabulary <- binomial_dtm$dimnames$Terms
binomial_train_dtm <- DocumentTermMatrix(
cleaned_binomial_docs,
control=list(
dictionary=observed_vocabulary
)
)
binomial_train_labels <- (
binomial_labels == 'comp.graphics'
) * 1
binomial_model <- glmnet(
binomial_train_dtm,
binomial_train_labels,
family='binomial'
)
binomial_train_dtm <- DocumentTermMatrix(
cleaned_binomial_docs,
control = list(dictionary = observed_vocabulary)
)
binomial_train_labels <- (binomial_labels == 'comp.graphics') * 1
library(glmnet)
binomial_model <- glmnet(
as.matrix(binomial_train_dtm),
binomial_train_labels,
family = 'binomial'
)
binomial_dtm
binomial_train_dtm
dim(binomial_train_dtm)
observed_vocabulary <- unlist(binomial_dtm$dimnames['Terms'])
observed_vocabulary <- Terms(binomial_dtm)
observed_vocabulary <- binomial_dtm$dimnames$Terms
length(observed_vocabulary)
binomial_train_dtm <- DocumentTermMatrix(
cleaned_binomial_docs,
control = list(dictionary = observed_vocabulary)
)
dim(binomial_train_dtm)
library(glmnet)
binomial_model <- glmnet(
as.matrix(binomial_train_dtm),
binomial_train_labels,
family = 'binomial'
)
